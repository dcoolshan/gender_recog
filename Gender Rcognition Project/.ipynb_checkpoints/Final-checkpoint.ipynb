{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('voice.csv') #loading data using panda \n",
    "male_data=data[data['label']=='male'] #partitioning male data from our original data voice.csv\n",
    "female_data=data[data['label']=='female'] #partitioning female data from our original data voice.csv\n",
    "male_data.loc[:,'label']=1  #converting label male to 1\n",
    "female_data.loc[:,'label']=0 #converting label female to 0\n",
    "data_with_label_1_0=pd.concat([male_data,female_data]) #combining male and female data again to make complete data again with label 0(female) and 1(male) \n",
    "data_with_label_1_0 = data_with_label_1_0.sample(frac=1).reset_index(drop=True) #shuffling our data randomly\n",
    "size_of_data=data_with_label_1_0.shape[0] #total size of data\n",
    "train_data_with_label_1_0=data_with_label_1_0.loc[:size_of_data*8.0/10,:] #taking 80% for train\n",
    "valid_data_with_label_1_0=data_with_label_1_0.loc[size_of_data*8.0/10:size_of_data*9.0/10,:] #taking 10% for validation\n",
    "test_data_with_label_1_0=data_with_label_1_0.loc[size_of_data*9.0/10:size_of_data,:] #taking 10% for test\n",
    "\n",
    " #converting our data from pandas to tensor\n",
    "train_data_tensor = torch.tensor(train_data_with_label_1_0.values)\n",
    "valid_data_tensor = torch.tensor(valid_data_with_label_1_0.values)\n",
    "test_data_tensor = torch.tensor(test_data_with_label_1_0.values)\n",
    "\n",
    "#making data loader for all train ,test and valid dataset and making batches for stochastic \n",
    "trainloader = torch.utils.data.DataLoader(train_data_tensor, batch_size=32, shuffle=False)\n",
    "validloader = torch.utils.data.DataLoader(valid_data_tensor, batch_size=317, shuffle=False) # 317 is full size of valid_data_tensor\n",
    "testloader = torch.utils.data.DataLoader(test_data_tensor, batch_size=316, shuffle=False)  # 316 is full size of test_data_tensor\n",
    "#basically taking whole batch together for valid and test to speed up process\n",
    "\n",
    "model = nn.Sequential(nn.Linear(20, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.Linear(64, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.Linear(256, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.Linear(256, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.25),\n",
    "                      nn.Linear(64, 1),\n",
    "                      nn.Sigmoid()\n",
    "                      ) \n",
    "#our model consist of 4 hidden layer one output and one input , 20 units in input,64 in first hidden,256 in second,256 in third,64 in fourth and one unit in output layer \n",
    "#and finally passed through sigmoid to get output between 0 and 1 \n",
    "\n",
    "train_losses, valid_losses,valid_accuracies= [], [], [] #will keep taking/appending values of this for plotting graph in end\n",
    "criterion = nn.BCELoss() #loss function used is BCE\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #adam optimizer is used as it prevents local minima convergence\n",
    "epochs = 250 #no. of epochs decided by observing validation and training graph convergence \n",
    "for e in range(epochs):\n",
    "    running_loss = 0 \n",
    "    for train_data_with_label in trainloader:   \n",
    "        train_data=train_data_with_label[:,:-1]\n",
    "        train_data_label=train_data_with_label[:,-1:]\n",
    "        optimizer.zero_grad()  #used to remove all previous accumulated values\n",
    "        \n",
    "        output = model(train_data.type(torch.FloatTensor)).view(-1) #precdicted value we get from network \n",
    "        train_data_label = train_data_label.view(-1)\n",
    "        #print(output)\n",
    "        loss = criterion(output,train_data_label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+= loss.item()\n",
    "    else:\n",
    "        valid_loss = 0\n",
    "        valid_accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for valid_data_with_label in validloader:\n",
    "                valid_data=valid_data_with_label[:,:-1]\n",
    "                valid_data_label=valid_data_with_label[:,-1:] \n",
    "                output_valid = model(valid_data.type(torch.FloatTensor)).view(-1) #precdicted value we get from network \n",
    "                #print(output_valid)\n",
    "                valid_data_label = valid_data_label.view(-1)\n",
    "                valid_loss += criterion(output_valid,valid_data_label.float())   \n",
    "                valid_accuracy+=len(np.where(valid_data_label.double()==(output_valid.detach()>0.5).double())[0])\n",
    "                       #valid_accuracy is total sum of correct prediction divided by total number of example, here we only do sum and down we do division \n",
    "        \n",
    "        model.train()\n",
    "        valid_accuracy=valid_accuracy/317 #as above we only did the sum of correct examples\n",
    "        train_losses.append(running_loss/len(trainloader)) #here we keep appending value of training loss after eacch epoch and later is used to make graph\n",
    "        valid_losses.append(valid_loss/len(validloader))  #here we keep appending value of validation loss after eacch epoch and later is used to make graph\n",
    "        valid_accuracies.append(valid_accuracy)  #here we keep appending value of validation accuracy after eacch epoch and later is used to make graph\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "              \"Valid Loss: {:.3f}.. \".format(valid_losses[-1]),\n",
    "              \"Valid Accuracy: {:.3f}.. \".format(valid_accuracy))\n",
    "\n",
    "        # training and validation complete now model has learnt all weights we require for good accuracy by adam optimizer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss') #plotting training loss vs no. of epoch\n",
    "plt.plot(valid_losses, label='Validation loss') #plotting validation loss vs no. of epoch\n",
    "plt.plot(valid_accuracies, label='Validation Accuracy') #plotting validation accuracy vs no. of epoch\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "test_loss=0\n",
    "test_accuracy=0\n",
    "for test_data_with_label in testloader:\n",
    "    test_data=test_data_with_label[:,:-1]\n",
    "   # print(test_data_label.shape)\n",
    "    test_data_label=test_data_with_label[:,-1:] \n",
    "    output_test = model(test_data.type(torch.FloatTensor)).view(-1)  #precdicted value we get from network \n",
    "        \n",
    "    test_data_label = test_data_label.view(-1)\n",
    "    test_accuracy+=len(np.where(test_data_label.double()==(output_test.detach()>0.5).double())[0])\n",
    "     #test_accuracy is total sum of correct prediction divided by total number of example, here we only do sum and down we do division \n",
    "    test_loss += criterion(output_valid,valid_data_label.float())\n",
    "   \n",
    "print(\"Test Accuracy: {:.3f}.. \".format( test_accuracy/316))    #total sum of correct/size of test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
